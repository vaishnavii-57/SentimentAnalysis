{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ0FKGKyb3ZQ",
        "outputId": "a1b3059a-8fca-4d78-afe6-977d31ee31db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                     0\n",
            "0    __label__2 Stuning even for the non-gamer: Thi...\n",
            "1    __label__2 The best soundtrack ever to anythin...\n",
            "2    __label__2 Amazing!: This soundtrack is my fav...\n",
            "3    __label__2 Excellent Soundtrack: I truly like ...\n",
            "4    __label__2 Remember, Pull Your Jaw Off The Flo...\n",
            "..                                                 ...\n",
            "995  __label__1 Fuzzy around the edges: I have only...\n",
            "996  __label__1 Brain Based Learning: The New Parad...\n",
            "997  __label__2 Brain based Learning: This is a tex...\n",
            "998  __label__1 Pop psychology at its worst: I find...\n",
            "999  __label__1 \"Science\": \"On average, we breathe ...\n",
            "\n",
            "[1000 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the text file using Pandas\n",
        "data = pd.read_csv('/content/train.ft.txt', header=None, delimiter='\\t')  # Assuming the data is tab-separated\n",
        "\n",
        "# Print the content of the file\n",
        "print(data.head(1000))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_data(data_point):\n",
        "    # Remove label prefix\n",
        "    data_point = re.sub(r'__label__\\d\\s+', '', data_point)\n",
        "\n",
        "    # Remove special characters, punctuation, and symbols\n",
        "    data_point = re.sub(r'[^A-Za-z0-9\\s]+', '', data_point)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    data_point = data_point.lower()\n",
        "\n",
        "    return data_point\n",
        "\n",
        "# Open the dataset file\n",
        "with open('train.ft.txt', 'r', encoding='utf-8') as file:\n",
        "    # Read lines from the file\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Initialize lists to store cleaned data points and their corresponding labels\n",
        "cleaned_data_points = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each line in the file\n",
        "for line in lines:\n",
        "    # Extract the label from the line\n",
        "    label = int(line.split()[0].split('__label__')[1])\n",
        "\n",
        "    # Extract the data point from the line (excluding the label)\n",
        "    data_point = ' '.join(line.split()[1:])\n",
        "\n",
        "    # Clean the data point\n",
        "    cleaned_data_point = clean_data(data_point)\n",
        "\n",
        "    # Append the cleaned data point and label to their respective lists\n",
        "    cleaned_data_points.append(cleaned_data_point)\n",
        "    labels.append(label)\n",
        "\n",
        "# Print the cleaned data points and their corresponding labels\n",
        "for cleaned_data_point, label in zip(cleaned_data_points, labels):\n",
        "    print(f\"Label: {label}, Cleaned Data Point: {cleaned_data_point}\")\n"
      ],
      "metadata": {
        "id": "s1whsSDgkqRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cleaned_dataset.txt', 'w', encoding='utf-8') as output_file:\n",
        "    # Write cleaned data points and labels to the new file\n",
        "    for cleaned_data_point, label in zip(cleaned_data_points, labels):\n",
        "        output_file.write(f\"{label} {cleaned_data_point}\\n\")\n",
        "\n",
        "print(\"Cleaned data saved to 'cleaned_dataset.txt'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4GamdvEnG4i",
        "outputId": "cad499a4-bc6a-470c-97e1-c3db6a896e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to 'cleaned_dataset.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import dump\n",
        "\n",
        "# Train a logistic regression classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels for the testing set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "dump(classifier, 'logistic_regression_model_3.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tX6PvFtp1HH",
        "outputId": "29d88864-e906-4b5b-a429-9525dd6a792e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8465163244698755\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['logistic_regression_model_3.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to clean and tokenize the text\n",
        "def clean_and_tokenize(text):\n",
        "    # Remove special characters, punctuation, and symbols\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the dataset\n",
        "with open('/content/cleaned_dataset.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Extract data points and labels\n",
        "data_points = []\n",
        "labels = []\n",
        "for line in lines:\n",
        "    label, text = line.split(' ', 1)\n",
        "    labels.append(int(label))\n",
        "    data_points.append(text.strip())\n",
        "\n",
        "# Clean and tokenize the data points\n",
        "tokenized_data = [clean_and_tokenize(text) for text in data_points]\n",
        "\n",
        "# Convert tokens into numerical vectors using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "X = tfidf_vectorizer.fit_transform(tokenized_data).toarray()\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
      ],
      "metadata": {
        "id": "G6-rST65CBfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb54d36-a66f-445f-ebf8-a7ed9e76163d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (11880, 1000) (11880,)\n",
            "Testing set shape: (2971, 1000) (2971,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from joblib import dump\n",
        "\n",
        "# Assuming tokenized_data is your tokenized text data\n",
        "\n",
        "# Create and fit the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "tfidf_vectorizer.fit(tokenized_data)\n",
        "\n",
        "# Save the TF-IDF vectorizer to a file\n",
        "dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISjIC6gzBEDh",
        "outputId": "008a9cd5-6ae8-4a5d-cc49-e994f16de2c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from joblib import load\n",
        "import numpy as np\n",
        "\n",
        "# Load the TF-IDF vectorizer from the file\n",
        "tfidf_vectorizer = load('tfidf_vectorizer.joblib')\n",
        "\n",
        "# Assuming tokenized_data and labels are loaded from somewhere\n",
        "\n",
        "# Transform tokenized data into TF-IDF features\n",
        "X = tfidf_vectorizer.transform(tokenized_data).toarray()\n",
        "y = np.array(labels)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "egQZoiBKAb76"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained classifier from the file\n",
        "loaded_classifier = load('logistic_regression_model_3.joblib')\n",
        "\n",
        "# Concatenate X_test and y_pred horizontally to match the original input format\n",
        "X_combined = np.hstack((X_test[:, :-1], y_pred.reshape(-1, 1)))  # Assuming the last column in X_test is the label column\n",
        "\n",
        "# Use the loaded classifier to make predictions\n",
        "new_data_predictions = loaded_classifier.predict(X_combined)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions:\", new_data_predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg2J2mCTGjUK",
        "outputId": "dbd22712-3b3e-4ce7-c2c0-f167e35d4817"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 2 1 ... 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyMkcLxbLzwh",
        "outputId": "ad4760d2-cbf1-4608-a1a0-2259ed2110ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[30342,  5405],\n",
              "       [ 4874, 31804]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEzbsJn0MOb1",
        "outputId": "607c513f-9610-4513-ef3e-683bf371a56e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.86      0.85      0.86     27187\n",
            "           2       0.86      0.87      0.86     28051\n",
            "\n",
            "    accuracy                           0.86     55238\n",
            "   macro avg       0.86      0.86      0.86     55238\n",
            "weighted avg       0.86      0.86      0.86     55238\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "# Load the trained classifier from the file\n",
        "loaded_classifier = load('logistic_regression_model_3.joblib')\n",
        "\n",
        "# Prompt the user to enter the text for prediction\n",
        "user_input_text = input(\"Enter the text for prediction: \")\n",
        "\n",
        "# Load the TF-IDF vectorizer used during training\n",
        "vectorizer = load('tfidf_vectorizer.joblib')\n",
        "\n",
        "# Transform the input text using the loaded vectorizer\n",
        "user_input_vectorized = vectorizer.transform([user_input_text])\n",
        "\n",
        "# Ensure the input is reshaped to match the expected shape (2D array)\n",
        "user_input_vectorized = user_input_vectorized.toarray().reshape(1, -1)\n",
        "\n",
        "# Use the loaded classifier to make predictions on the user input\n",
        "new_data_predictions = loaded_classifier.predict(user_input_vectorized)\n",
        "\n",
        "# Print the predictions\n",
        "if new_data_predictions[0] == 1:\n",
        "    print(\"Result: This review is negative\")\n",
        "else:\n",
        "    print(\"Result: This review is positive\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL5bQ2vrBhsb",
        "outputId": "f15e4d0a-a3f8-4cce-ded6-6990056cfa51"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text for prediction: this is bad\n",
            "Result: This review is negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ja8Ve2nsCC72"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}